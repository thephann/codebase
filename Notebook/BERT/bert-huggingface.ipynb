{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport transformers\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import Subset\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-02T01:12:37.130696Z","iopub.execute_input":"2022-04-02T01:12:37.130992Z","iopub.status.idle":"2022-04-02T01:12:39.448591Z","shell.execute_reply.started":"2022-04-02T01:12:37.130915Z","shell.execute_reply":"2022-04-02T01:12:39.447816Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    MODEL_PATH = './model/'\n    SAVE_EVERT = 10\n    EPOCHS = 20\n    BATCH_SIZE = 32\n    LEARNING_RATE = 1e-5\n    TRAIN_TEST_SPLIT = 0.3","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:12:41.143138Z","iopub.execute_input":"2022-04-02T01:12:41.143540Z","iopub.status.idle":"2022-04-02T01:12:41.147563Z","shell.execute_reply.started":"2022-04-02T01:12:41.143501Z","shell.execute_reply":"2022-04-02T01:12:41.146886Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nprint(\"loading data\")\ndf = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n# train_df, valid_df = train_test_split(df, test_size=CONFIG.TRAIN_TEST_SPLIT, random_state=42)\n# test_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n# sub = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:12:43.928037Z","iopub.execute_input":"2022-04-02T01:12:43.928776Z","iopub.status.idle":"2022-04-02T01:12:44.058867Z","shell.execute_reply.started":"2022-04-02T01:12:43.928739Z","shell.execute_reply":"2022-04-02T01:12:44.058167Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Device: cuda\nloading data\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:12:46.498591Z","iopub.execute_input":"2022-04-02T01:12:46.498846Z","iopub.status.idle":"2022-04-02T01:12:46.527440Z","shell.execute_reply.started":"2022-04-02T01:12:46.498818Z","shell.execute_reply":"2022-04-02T01:12:46.526614Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Emoji patterns\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:12:50.577567Z","iopub.execute_input":"2022-04-02T01:12:50.577825Z","iopub.status.idle":"2022-04-02T01:12:50.585678Z","shell.execute_reply.started":"2022-04-02T01:12:50.577795Z","shell.execute_reply":"2022-04-02T01:12:50.584480Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import WordPunctTokenizer\nimport re\nimport emoji\nfrom bs4 import BeautifulSoup\nimport itertools\n\ntok = WordPunctTokenizer()\npat1 = r'@[A-Za-z0-9]+'\npat2 = r'https?://[A-Za-z0-9./]+'\n\n# ref: https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n    # removing UTF-8 BOM (Byte Order Mark)\n\ndef tweet_cleaner(text):\n    try:\n        text1 = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") # The UTF-8 BOM is a sequence of bytes (EF BB BF) that allows the reader to identify a file as being encoded in UTF-8\n    except:\n        text1 = text\n    \n    \n    #replace consecutive non-ASCII characters with a space\n    text1 = re.sub(r'[^\\x00-\\x7F]+',' ', text1)\n    \n    #remove emojis from tweet\n    text2 = emoji_pattern.sub(r'', text1)\n    \n    # Remove emoticons\n    # text3 = [word for word in text2.split() if word not in emoticons]\n    # text3 = \" \".join(text3)\n    \n    # contradictions and special characters \n    # text4 = spl_ch_contra(text3)\n    \n    # HTML encoding\n    soup = BeautifulSoup(text2, 'lxml') #HTML encoding has not been converted to text, and ended up in text field as ‘&amp’,’&quot’,etc.\n    text5 = soup.get_text()\n    \n    # removing @ mentions\n    text6 = re.sub(pat1, '', text5)\n    \n    # Removing URLs\n    text7 = re.sub(pat2, '', text6)\n    \n    # Removing punctuations\n    # text8 = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)\\[\\]\\\"\\'\\%\\*\\#\\@]\", \" \", text7)\n    \n    # Fix misspelled words\n    text9 = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text7))# checking that each character should occur not more than 2 times in every word\n\n    # Tokenizing ,change cases & join together to remove unneccessary white spaces\n    text9_list = tok.tokenize(text9.lower())\n    text10 = (\" \".join(text9_list)).strip()\n    \n    return text10","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:12:53.294753Z","iopub.execute_input":"2022-04-02T01:12:53.295146Z","iopub.status.idle":"2022-04-02T01:12:54.054134Z","shell.execute_reply.started":"2022-04-02T01:12:53.295093Z","shell.execute_reply":"2022-04-02T01:12:54.053412Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# cleaning tweets\ndf['text_cleaned'] = list(map(lambda x:tweet_cleaner(x),df['text']) )","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:12:57.455796Z","iopub.execute_input":"2022-04-02T01:12:57.456064Z","iopub.status.idle":"2022-04-02T01:12:59.335591Z","shell.execute_reply.started":"2022-04-02T01:12:57.456035Z","shell.execute_reply":"2022-04-02T01:12:59.334901Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:14:23.687136Z","iopub.execute_input":"2022-04-02T01:14:23.687848Z","iopub.status.idle":"2022-04-02T01:14:23.699945Z","shell.execute_reply.started":"2022-04-02T01:14:23.687807Z","shell.execute_reply":"2022-04-02T01:14:23.699156Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         id              keyword                 location  \\\n7386  10570            windstorm                  Houston   \n867    1252                blood                      NaN   \n7210  10329               weapon                      NaN   \n4005   5688               floods     Ogba, Lagos, Nigeria   \n24       36                  NaN                      NaN   \n1198   1724  buildings%20burning                Quincy MA   \n7084  10146             upheaval                    INDIA   \n6285   8978                storm                 NC || OR   \n7357  10534             wildfire  Bakersfield, California   \n2406   3464             derailed           Kwara, Nigeria   \n\n                                                   text  target  \\\n7386       newroofandhardyupwindstorminspectiontomorrow       0   \n867                    scotto519happybirthdayyoungblood       0   \n7210                                   weaponscatalogue       0   \n4005  apcchieftaintasksdicksononn15bfloodsdonationto...       1   \n24                                             looooool       0   \n1198  dougmartin17firemanedrunsintoburningbuildingsw...       1   \n7084  lyfneedsqualityandacertainsenseofsecuritybeing...       0   \n6285                icecreamcupcakewarsstormcontentsara       0   \n7357  #california#wildfiredestroysmorehomesbutcrewsa...       1   \n2406  ofwhatuseexactlyisthenationalassembly?honestly...       0   \n\n                                           text_cleaned  \n7386  new roof and hardy up .. windstorm inspection ...  \n867                          happy birthday young blood  \n7210                             weapon ' s catalogue ~  \n4005  apc chieftain tasks dickson on n15b floods don...  \n24                                                 lool  \n1198  fireman ed runs into burning buildings while o...  \n7084  lyf needs quality and a certain sense of secur...  \n6285    ice cream + cupcake wars + storm = content sara  \n7357  # california # wildfire destroys more homes bu...  \n2406  of what use exactly is the national assembly ?...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7386</th>\n      <td>10570</td>\n      <td>windstorm</td>\n      <td>Houston</td>\n      <td>newroofandhardyupwindstorminspectiontomorrow</td>\n      <td>0</td>\n      <td>new roof and hardy up .. windstorm inspection ...</td>\n    </tr>\n    <tr>\n      <th>867</th>\n      <td>1252</td>\n      <td>blood</td>\n      <td>NaN</td>\n      <td>scotto519happybirthdayyoungblood</td>\n      <td>0</td>\n      <td>happy birthday young blood</td>\n    </tr>\n    <tr>\n      <th>7210</th>\n      <td>10329</td>\n      <td>weapon</td>\n      <td>NaN</td>\n      <td>weaponscatalogue</td>\n      <td>0</td>\n      <td>weapon ' s catalogue ~</td>\n    </tr>\n    <tr>\n      <th>4005</th>\n      <td>5688</td>\n      <td>floods</td>\n      <td>Ogba, Lagos, Nigeria</td>\n      <td>apcchieftaintasksdicksononn15bfloodsdonationto...</td>\n      <td>1</td>\n      <td>apc chieftain tasks dickson on n15b floods don...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>36</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>looooool</td>\n      <td>0</td>\n      <td>lool</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>1724</td>\n      <td>buildings%20burning</td>\n      <td>Quincy MA</td>\n      <td>dougmartin17firemanedrunsintoburningbuildingsw...</td>\n      <td>1</td>\n      <td>fireman ed runs into burning buildings while o...</td>\n    </tr>\n    <tr>\n      <th>7084</th>\n      <td>10146</td>\n      <td>upheaval</td>\n      <td>INDIA</td>\n      <td>lyfneedsqualityandacertainsenseofsecuritybeing...</td>\n      <td>0</td>\n      <td>lyf needs quality and a certain sense of secur...</td>\n    </tr>\n    <tr>\n      <th>6285</th>\n      <td>8978</td>\n      <td>storm</td>\n      <td>NC || OR</td>\n      <td>icecreamcupcakewarsstormcontentsara</td>\n      <td>0</td>\n      <td>ice cream + cupcake wars + storm = content sara</td>\n    </tr>\n    <tr>\n      <th>7357</th>\n      <td>10534</td>\n      <td>wildfire</td>\n      <td>Bakersfield, California</td>\n      <td>#california#wildfiredestroysmorehomesbutcrewsa...</td>\n      <td>1</td>\n      <td># california # wildfire destroys more homes bu...</td>\n    </tr>\n    <tr>\n      <th>2406</th>\n      <td>3464</td>\n      <td>derailed</td>\n      <td>Kwara, Nigeria</td>\n      <td>ofwhatuseexactlyisthenationalassembly?honestly...</td>\n      <td>0</td>\n      <td>of what use exactly is the national assembly ?...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class BERT(nn.Module):\n    def __init__(self, bert_model_name, num_labels, dropout=0.1, freeze_bert=True):\n        super(BERT, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(bert_model_name)\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(768, num_labels)\n        self.num_labels = num_labels\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        _, pooled_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.696386Z","iopub.execute_input":"2022-04-01T19:26:47.696645Z","iopub.status.idle":"2022-04-01T19:26:47.704673Z","shell.execute_reply.started":"2022-04-01T19:26:47.696609Z","shell.execute_reply":"2022-04-01T19:26:47.703272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DisasterTweetsDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=512, train=True):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.df = df\n        self.train = train\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.iloc[idx][\"text_cleaned\"])\n        if self.train:\n            targets = torch.tensor(self.df.iloc[idx][\"target\"], dtype=torch.long)\n        \n        inputs = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            return_tensors=\"pt\",\n        )\n        input_ids, attention_mask, token_type_ids = inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]\n        if self.train:\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"token_type_ids\": token_type_ids,\n                \"targets\": targets,\n                }\n        else:\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"token_type_ids\": token_type_ids,\n                }","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.706502Z","iopub.execute_input":"2022-04-01T19:26:47.707116Z","iopub.status.idle":"2022-04-01T19:26:47.718035Z","shell.execute_reply.started":"2022-04-01T19:26:47.707083Z","shell.execute_reply":"2022-04-01T19:26:47.717316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.721932Z","iopub.execute_input":"2022-04-01T19:26:47.72222Z","iopub.status.idle":"2022-04-01T19:26:47.734314Z","shell.execute_reply.started":"2022-04-01T19:26:47.722189Z","shell.execute_reply":"2022-04-01T19:26:47.733641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.735637Z","iopub.execute_input":"2022-04-01T19:26:47.736087Z","iopub.status.idle":"2022-04-01T19:26:47.747015Z","shell.execute_reply.started":"2022-04-01T19:26:47.736052Z","shell.execute_reply":"2022-04-01T19:26:47.746397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n#helper function to get train and val data loaders for each fold \ndef get_data_loaders(dataset,train_indexes,val_indexes):\n    train_tensor = Subset(dataset,train_indexes)\n    val_tensor = Subset(dataset,val_indexes)\n    train_dataloader = DataLoader(\n            train_tensor, \n            sampler = RandomSampler(train_tensor), \n            batch_size = CONFIG.BATCH_SIZE\n        )\n\n    val_dataloader = DataLoader(\n            val_tensor, \n            sampler = SequentialSampler(val_tensor), \n            batch_size = CONFIG.BATCH_SIZE \n        )\n    return train_dataloader,val_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.748435Z","iopub.execute_input":"2022-04-01T19:26:47.748899Z","iopub.status.idle":"2022-04-01T19:26:47.755129Z","shell.execute_reply.started":"2022-04-01T19:26:47.748866Z","shell.execute_reply":"2022-04-01T19:26:47.754393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_folds = 6\ncurrent_fold = -1\nall_folds_preds = []\nepochs = 1\nfold=StratifiedKFold(n_splits=total_folds, shuffle=True, random_state=42)\n\ntraining_stats = []","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.756446Z","iopub.execute_input":"2022-04-01T19:26:47.756878Z","iopub.status.idle":"2022-04-01T19:26:47.763776Z","shell.execute_reply.started":"2022-04-01T19:26:47.756845Z","shell.execute_reply":"2022-04-01T19:26:47.763128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.764897Z","iopub.execute_input":"2022-04-01T19:26:47.765282Z","iopub.status.idle":"2022-04-01T19:26:47.772602Z","shell.execute_reply.started":"2022-04-01T19:26:47.765247Z","shell.execute_reply":"2022-04-01T19:26:47.771839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.77476Z","iopub.execute_input":"2022-04-01T19:26:47.776127Z","iopub.status.idle":"2022-04-01T19:26:47.781976Z","shell.execute_reply.started":"2022-04-01T19:26:47.776091Z","shell.execute_reply":"2022-04-01T19:26:47.78124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, train_dataloader, device):\n    t0 = time.time()\n    model.train()\n    total_loss = 0.0\n    correct_predictions = 0.0\n#     bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n    for data in train_dataloader:\n        optimizer.zero_grad()\n        input_ids = data[\"input_ids\"].to(device).squeeze(1)\n        attention_mask = data[\"attention_mask\"].to(device).squeeze(1)\n        token_type_ids = data[\"token_type_ids\"].to(device).squeeze(1)\n        targets = data[\"targets\"].to(device).unsqueeze(1)\n#         print(targets.shape)\n        out = model(input_ids, attention_mask, token_type_ids)\n        \n        loss = criterion(out, targets.float())\n#         bar.set_postfix({\n#                 \"Train Loss\": \"{:.6f}\".format(abs(loss)),}\n#         )\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n    training_time = format_time(time.time() - t0)\n    return total_loss / len(train_dataloader), training_time","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:31:35.792056Z","iopub.execute_input":"2022-04-01T19:31:35.79253Z","iopub.status.idle":"2022-04-01T19:31:35.800112Z","shell.execute_reply.started":"2022-04-01T19:31:35.792491Z","shell.execute_reply":"2022-04-01T19:31:35.799229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_one_epoch(model, valid_dataloader, device):\n    t0 = time.time()\n    model.eval()\n    total_loss = 0.0\n    correct_predictions = 0.0\n    with torch.no_grad():\n#         bar = tqdm(enumerate(valid_dataloader), total=len(valid_dataloader))\n        for data in valid_dataloader:\n            input_ids = data[\"input_ids\"].to(device).squeeze(1)\n            attention_mask = data[\"attention_mask\"].to(device).squeeze(1)\n            token_type_ids = data[\"token_type_ids\"].to(device).squeeze(1)\n            targets = data[\"targets\"].to(device).unsqueeze(1)\n            out = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(out, targets.float())\n#             bar.set_postfix({\n#                 \"Valid Loss\": \"{:.6f}\".format(abs(loss)),}\n#             )\n            total_loss += loss.item()\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    return total_loss / len(valid_dataloader),validation_time","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:31:33.032688Z","iopub.execute_input":"2022-04-01T19:31:33.033245Z","iopub.status.idle":"2022-04-01T19:31:33.041231Z","shell.execute_reply.started":"2022-04-01T19:31:33.033205Z","shell.execute_reply":"2022-04-01T19:31:33.040432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloader(train_dataset, valid_dataset, batch_size=CONFIG.BATCH_SIZE):\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    return train_dataloader, valid_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:26:47.817214Z","iopub.execute_input":"2022-04-01T19:26:47.817782Z","iopub.status.idle":"2022-04-01T19:26:47.827162Z","shell.execute_reply.started":"2022-04-01T19:26:47.817747Z","shell.execute_reply":"2022-04-01T19:26:47.826288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for each fold..\nfor train_index, test_index in fold.split(df,df['target']):\n    model = BERT(\"../input/huggingface-bert/bert-base-cased\", num_labels=1, dropout=0.1).to(device)\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/huggingface-bert/bert-base-cased\")\n    dataset = DisasterTweetsDataset(df, tokenizer, max_len=64, train=True)\n    optimizer = transformers.AdamW(model.parameters(), lr = 1e-5,eps = 1e-8)\n    current_fold = current_fold+1\n    scheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=len(dataset) * CONFIG.EPOCHS,\n    )\n    current_fold = current_fold+1\n    criterion = nn.BCEWithLogitsLoss()\n    train_dataloader,validation_dataloader = get_data_loaders(dataset,train_index,test_index)\n    \n    best_valid_loss = float(\"inf\")\n    for epoch in range(CONFIG.EPOCHS):\n        print(f'Epoch {epoch + 1}/{CONFIG.EPOCHS}')\n        print('-' * 10)\n        \n        train_loss, training_time = train_one_epoch(model, optimizer, scheduler, train_dataloader, device)\n        print(f'Train loss {train_loss} Training time {training_time}')\n        valid_loss, validation_time = valid_one_epoch(model, validation_dataloader, device)\n        print(f'Val loss {valid_loss} Validation time {validation_time}')\n        \n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), \"best_model.bin\")  ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T19:38:37.113301Z","iopub.execute_input":"2022-04-01T19:38:37.113571Z"},"trusted":true},"execution_count":null,"outputs":[]}]}